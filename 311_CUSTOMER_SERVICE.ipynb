{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Data importation\n",
    "df = pd.read_csv(\"311_Service_Requests_from_2010_to_Present.csv\", low_memory=False, dtype={48:'str', 49:'str'})\n",
    "\n",
    "### This dataset specified that columns 48, and 49 have mixtypes to resolve this warning,\n",
    "#I specified the data types explicitly for these columns during \n",
    "#the import using the dtype parameter  passing low_memoery=False in the read_csv() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 Previewing the head of the dataset\n",
    "#visualizing the head of the dataset gives us an overview of the overall dataset.\n",
    "#However, the visualization processes requires the use of charts.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the tail of the dataset gives us an overview of the overall dataset.\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 DataFrame Columns\n",
    "#df.columns populates the column names in the dataset, it is relevant for accessing, \n",
    "#manipulating, and working with column names in a pandas DataFrame. \n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 Shape of dataset\n",
    "# df.shape is a valuable attribute, I was able to understand the dimensions of a DataFrame. \n",
    "# It aided in data validation,indexing and manipulation tasks.\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.5 Identifying variables with null values\n",
    "\n",
    "# Populating columns with NaN values, I used the .isnull() function on the dataframe, boolean mask is generated,\n",
    "#where True indicates the presence of null values in the cBorresponding cells and False a datapoint. \n",
    "# .sum() function helps in presenting the output as a series.\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values percentage wise.\n",
    "df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic data exploratory analysis:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 frequency plot to show the number of null values in each column of the DataFrame\n",
    "\n",
    "columns_with_null_values = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "# Calculate frequency of null values in each column using .sum()function on .isnull() and applying it to df[columns_with_null_values]\n",
    "Null_Frequency = df[columns_with_null_values].isnull().sum()\n",
    "\n",
    "#plt.figure was used to specify the size of the chart.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# .Plot was imployed to plot the NaN in each column.\n",
    "Null_Frequency.plot(kind='bar', color='g', lw=0.2, alpha=0.5 )\n",
    "\n",
    "# the x and y cordinates.\n",
    "plt.title=('columns with null values')\n",
    "plt.xlabel=('Columns')\n",
    "plt.ylabel=('Frequency')\n",
    "\n",
    "#plt.show was used to display the plot of columns_with_null_values\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 Missing value treatment,\n",
    "\n",
    "## Set the threshold percentage\n",
    "Threshold = 70 \n",
    "\n",
    "missing_percentage = df.isnull().mean() * 100 #Percentatge calculation of columns with missing values.\n",
    "\n",
    "## Get the column names where the null percentage is greater than the threshold\n",
    "columns_to_drop = missing_percentage[missing_percentage > Threshold].index\n",
    "\n",
    "dropped_columns = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2.1 Records whose closed date values are null\n",
    "\n",
    "# .isnull() is applied to closed date, which outputs each element as True if the corresponding value in df is null and False otherwise.\n",
    "\n",
    "#.dropna() dropped the null values in the closed date column,leaving False values that are true values. \n",
    "\n",
    "closed_date_with_null = df['Closed Date'].isnull().dropna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_date_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .dropna() is applied on the df and specifies the subset parameter as 'Closed Date'. \n",
    "\n",
    "#The inplace=True parameter is used to modify the DataFrame,rather than creating a new DataFrame. \n",
    "\n",
    "date = df.dropna(subset=['Closed Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3\n",
    "#df['Created Date'] selects the 'Created Date' column of the DataFrame df.\n",
    "\n",
    "#pd.to_datetime() function is used to convert the created date to datetime64[ns] format,\n",
    "#it assigns the converted datetime values back to the 'Created Date' column, overwriting the previous values.\n",
    "\n",
    "\n",
    "df['Created Date'] = pd.to_datetime(df['Created Date'])\n",
    "df['Created Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Closed Date'] = pd.to_datetime(df['Closed Date'])\n",
    "df['Closed Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3.1 Elapsed date and time\n",
    "\n",
    "#(df['Closed Date']) is subtracted from (df['Created Date']) using simple arithmetic \n",
    "#to obtain the time difference in closing each complaint type. \n",
    "\n",
    "elapsed_time = (df['Closed Date'])-(df['Created Date'])\n",
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3.2 Convert elapsed time to seconds\n",
    "\n",
    "#dt.seconds is a property of the dt accessor that returns the elapsed time in seconds when applied.\n",
    "\n",
    "Convert_seconds = pd.DataFrame(elapsed_time.dt.seconds)\n",
    "Convert_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3.3 descriptive statistics\n",
    "\n",
    "#.describe() is a descriptive statistics for the variable Convert_seconds, which represents the elapsed time in seconds. \n",
    "# It provides a summary of various statistical measures.\n",
    "\n",
    "Convert_seconds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.4 Null values in complaint type and city columns\n",
    "#.isnull() applied to the complaint type and city column which returns a boolean value as True if there is a missing value,\n",
    "#while the.sum() It calculates the sum of True values (missing values) along each column.\n",
    "\n",
    "null_values = df[['Complaint Type','City']].isnull().sum()\n",
    "null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.5 NaN values in the city column was replaced with Unkown city using .replace() function placed in a dictionary.\n",
    "\n",
    "df.replace({'City': np.NaN}, 'Unkown City', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This  line of code was executed to confirm the absence of null values.\n",
    "df['City'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "City_Complaint_Counts= df.groupby('City')['Complaint Type'].value_counts()\n",
    "City_Complaint_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "# 2.3.6 Plotting each city's complaint frequencies\n",
    "\n",
    "# df.groupby function was used to group the city column by complaints Type column, \n",
    "#this enabled us to understand specific complaint types associated to each cities.\n",
    "#value_counts() was necessarry to understand the total number of specific complaint types for each cities.\n",
    "City_Complaint_Counts = df.groupby('City')['Complaint Type'].value_counts()\n",
    "\n",
    "\n",
    "# I initiated a for loop to iterates through the city column in the df.\n",
    "#.unique() is a pandas method applied to the 'City' column that returns an array containing only\n",
    "#the unique values from that Series. It eliminates any duplicate values.\n",
    "for city in df['City'].unique():\n",
    "    \n",
    "#['City'].plot In this case, it is used to create a bar plot for the selected series.\n",
    "#kind='bar' specifies the type of plot to create, which is a bar plot in this case.\n",
    "#alpha=0.7 sets the transparency level of the bars to 0.7, making them slightly transparent.\n",
    "    City_Complaint_Counts[city].plot(kind='bar', color='green', alpha=0.7, edgecolor='white', linewidth=1.2)\n",
    "    \n",
    "        \n",
    "#The lines plt.xlabel=('Complaint Type') and plt.ylabel=('Frequency') are used to set the labels for\n",
    "#the x-axis and y-axis, respectively, in a matplotlib plot.\n",
    "    plt.xlabel=('Complaint Type')\n",
    "    plt.ylabel=('Frequency')\n",
    "   \n",
    " \n",
    "# Displays the graphical representation of the dataset   \n",
    "    plt.show()   \n",
    "    \n",
    "    print(\"City:\", city)  # Print the city name\n",
    "  \n",
    "    counter += 1 # The counter helps us to populate the desired number of cities.\n",
    "    if counter >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing specific city's complaint frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_type = df['Complaint Type']\n",
    "complaint_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3.7 SCATTER PLOT OF THE CONCENTRATION OF COMPLAINT ACROSS BROOKLYN\n",
    "\n",
    "#creates a new DataFrame called brooklyn by filtering the original DataFrame df based on a condition df['City'] == 'BROOKLYN' \n",
    "# and returning a boolean value True for Brooklyn in the city column.\n",
    "brooklyn = df[df['City'] == 'BROOKLYN']\n",
    "\n",
    "#plt.figure was used to determine the size of the graph width wise and height alike.\n",
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "# for loop iterates over each unique value in the brooklyn dataframe Complaint Type column\n",
    "#using the .unique()function.\n",
    "for complaint_type in brooklyn['Complaint Type'].unique():\n",
    "\n",
    "# applies the boolean mask to the DataFrame brooklyn, returning a new DataFrame that contains only \n",
    "#the rows where the condition is True.     \n",
    "    data = brooklyn[brooklyn['Complaint Type'] == complaint_type]\n",
    "    \n",
    "#This line plots a scatter plot using the 'Longitude' and 'Latitude' columns of the DataFrame data.    \n",
    "    plt.scatter(data['Longitude'], data['Latitude'], s=10, alpha=0.5, label=str(complaint_type))\n",
    "    \n",
    "#Title of the Scatter plot    \n",
    "plt.title=('Scatter Plot of Complaint Concentration in Brooklyn')\n",
    "#Label of the x axis\n",
    "plt.xlabel=('Longitude')\n",
    "#Label of the y axis\n",
    "plt.ylabel=('Latitude')\n",
    "# provides additional information about the mapping of colors to data values in the plot,\n",
    "#for easy understanding of the distribution of complaint type across Brooklyn.\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Displays the graphical representation of the dataset\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point in the plot represents an individual complaint, with its location determined by the latitude and longitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3.7 HEXBIN PLOT OF THE CONCENTRATION OF COMPLAINT ACROSS BROOKLYN\n",
    "\n",
    "##plt.figure was used to determine the size of the graph width wise and height alike.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#This line plots a Hexbin plot using the 'Longitude' and 'Latitude' columns of the DataFrame data. \n",
    "plt.hexbin(data['Longitude'], data['Latitude'], gridsize=20, cmap='viridis')\n",
    "\n",
    "\n",
    "\n",
    "plt.title=('Hexbin plot of Complaint Concentration in Brooklyn')\n",
    "#Label of the x axis\n",
    "plt.xlabel=('Longitude')\n",
    "#Label of the y axis\n",
    "plt.ylabel=('Latitude')\n",
    "#adds a colorbar to a plot created using Matplotlib. The colorbar is a rectangular color scale that provides additional\n",
    "#information about the mapping of colors to data values in the plot.\n",
    "plt.colorbar(label='Count')\n",
    "\n",
    "# Displays the graphical representation of the dataset\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hexbin plot visually represents the spatial distribution and concentration of complaints across Brooklyn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 major types of complaints\n",
    "#calculates the frequency of each unique value in the 'Complaint Type' column of the DataFrame df and stores the result\n",
    "#in the variable major_types_of_complaints.\n",
    "major_types_of_complaints = df['Complaint Type'].value_counts()\n",
    "major_types_of_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 A BAR CHART PLOT SHOWING COMPLAINTS TYPES IN DECENDING ORDER\n",
    "\n",
    "# After getting the unique value in the 'Complaint Type' column \n",
    "major_types_of_complaints = df['Complaint Type'].value_counts()\n",
    "\n",
    "#plt.figure was used to determine the size of the plot.\n",
    "plt.figure(figsize=(11, 7))\n",
    "\n",
    "#plt.bar creates a bar plot using the values and index from the major_types_of_complaints Series,\n",
    "plt.bar(major_types_of_complaints.index, major_types_of_complaints.values, lw=1.2, color='pink', label='Count')\n",
    "\n",
    "plt.title=('major types of complaints')\n",
    "\n",
    "plt.xlabel=('Complaint Type')\n",
    "plt.ylabel=('Count')\n",
    "\n",
    "#to specify the rotation angle for the tick labels. In this case, the angle is set to 90 degrees, \n",
    "#causing the tick labels to rotate vertically.\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "# Displays the graphical representation of the dataset.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph clearly displays the frequency of the major complaint types in our DataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 FREQUENCY OF VARIOUS TYPES OF COMPLAINTS FOR NEW YORK\n",
    "\n",
    "#This line filters the DataFrame df based on a condition. It creates a new DataFrame called\n",
    "#Frequency_NewYork_Complaints that includes only the rows where the 'City' column has the value 'NEW YORK'.\n",
    "Frequency_NewYork_Complaints = df[df['City'] == 'NEW YORK']\n",
    "\n",
    "\n",
    "#This line calculates the frequency of each unique combination of 'City' and 'Complaint Type' in the DataFrame df using .value_counts\n",
    "#The groupby() method is used to group the data by the 'City' column, and then value_counts() is applied to the \n",
    "#'Complaint Type' column within each group..\n",
    "city_complaint_counts = df.groupby('City')['Complaint Type'].value_counts()\n",
    "\n",
    "#This line assigns the string value 'NEW YORK' to the variable specific_city.\n",
    "#indicating the specific city for which I want to retrieve the complaint counts.\n",
    "specific_city = 'NEW YORK'    \n",
    "\n",
    "\n",
    "#This line accesses the specific complaint counts for NEW YORK from the city_complaint_counts Series.\n",
    "city_complaint_counts[specific_city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3 TOP TEN COMPLAINT TYPES\n",
    "\n",
    "# This line extracts the 'Complaint Type' column from the DataFrame df and assigns it to the variable complaints.\n",
    "\n",
    "complaints = df['Complaint Type']\n",
    "\n",
    "#.value_counts calculates the frequency of each unique value in the complaints Series.\n",
    "#.head() is chained to the resulting Series to select the top 10 most frequent complaint types\n",
    "top_ten = complaints.value_counts().head(10)\n",
    "\n",
    "#This prints out the top ten complaint types from the df.\n",
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4 display various types of complaints for each city\n",
    "\n",
    "# he groupby() method is called on df with 'City' as the grouping column.\n",
    "#.unique() function is applied to the 'Complaint Type' column within each city subgroups  based on the distinct values in the 'City'. \n",
    "#This allows us to obtain the unique complaint types for each city\n",
    "grouped = df.groupby('City')['Complaint Type'].unique()\n",
    "\n",
    "#This line sorts the pandas Series grouped by its index (city names) in ascending order.\n",
    "grouped = grouped.sort_index()\n",
    "\n",
    "#Initializes the counter\n",
    "counter = 0\n",
    "\n",
    "#This line initiates a loop that iterates over the items of the grouped object.\n",
    "for city, complaint_types in grouped.items():\n",
    "\n",
    "#This line prints the name of the current city using an f-string.    \n",
    "    print(f\"City: {city}\")\n",
    "\n",
    "#This line initiates another loop that iterates over each element in the complaint_types array.    \n",
    "    for complaint_type in complaint_types:\n",
    "\n",
    "#This line prints the current complaint type using an f-string        \n",
    "        print(f\"- {complaint_type}\")\n",
    "        \n",
    "        \n",
    "        counter += 1 # Increment the counter after processing each citys.\n",
    "        \n",
    "    if counter >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 DataFrame, df_new, which contains cities as columns and complaint types in rows\n",
    "\n",
    "#df['City'].value_counts() calculates the frequency count of each unique value in the 'City' column of the df.\n",
    "#.head(10) selects the top 10 most frequent values from the resulting counts.\n",
    "#.index retrieves the index (i.e., the unique city names) from the resulting Series.\n",
    "#.to_list() converts the index to a list.\n",
    "cities=df['City'].value_counts().head(10).index.tolist()\n",
    "\n",
    "#isin()checks for each value in the 'City' column of df if it is present in the cities list.\n",
    "pop_cities= df[df.City.isin(cities)]\n",
    "\n",
    "#pd.crosstab() is a function from the pandas library that computes a cross-tabulation \n",
    "#between two or more variables.\n",
    "df_new=pd.crosstab(pop_cities['City'],pop_cities['Complaint Type'])\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the major types of complaints\n",
    "\n",
    "#This line groups the DataFrame df by both the 'City' and 'Complaint Type' columns.\n",
    "grouped = df.groupby(['City', 'Complaint Type'])\n",
    "\n",
    "#.size() is applied to the grouped object, returning a Series that contains the count\n",
    "#of occurrences for each combination of 'City' and 'Complaint Type'.\n",
    "complaint_counts = grouped.size()\n",
    "\n",
    "#reset_index() removes the multi-level index and assigns a new integer index to the DataFrame.\n",
    "complaint_counts = complaint_counts.reset_index()\n",
    "\n",
    "#This line sorts the complaint_counts DataFrame by the 'City' column in ascending order\n",
    "#and the count values (0) in descending order.\n",
    "complaint_counts = complaint_counts.sort_values(['City', 0], ascending=[True, False])\n",
    "\n",
    "#his line groups the complaint_counts DataFrame by the 'City' column.\n",
    "major_complaints = complaint_counts.groupby('City').first()\n",
    "\n",
    "#.reset_index() removes the existing index and assigns a new integer index to the DataFrame.\n",
    "major_complaints = major_complaints.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 CHART SHOWING TYPES OF COMPLAINTS IN EACH CITY\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "# Group the DataFrame by 'City' and 'Complaint Type' and count the occurrences\n",
    "grouped = df.groupby(['City', 'Complaint Type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Get the list of cities and complaint types\n",
    "cities = grouped.index\n",
    "complaint_types = grouped.columns\n",
    "\n",
    "# Set the color palette for different complaint types\n",
    "colors = sns.color_palette('Set3', len(complaint_types))\n",
    "\n",
    "# Plot the chart\n",
    "plt.figure(figsize=(15, 10))\n",
    "bottom = None  # Variable to keep track of the bottom values for stacked bars\n",
    "\n",
    "# Iterate over each complaint type and plot the stacked bars for each city\n",
    "for i, complaint_type in enumerate(complaint_types):\n",
    "    values = grouped[complaint_type]\n",
    "    plt.bar(cities, values, bottom=bottom, color=colors[i], label=complaint_type)\n",
    "    \n",
    "    \n",
    "#This conditional statement updates the bottom variable. If bottom is None, it assigns it the current values. \n",
    "#Otherwise, it adds the current values to the existing bottom values.    \n",
    "    if bottom is None:\n",
    "        bottom = values\n",
    "    else:\n",
    "        bottom += values\n",
    "\n",
    "        \n",
    "# Set the chart title, labels, and legend\n",
    "plt.title=('Number of Complaint Types per City')\n",
    "plt.xlabel=('City')\n",
    "plt.ylabel=('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right', title='Complaint Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can infer that the cities with most frequent complaints are, Brooklyn, New York, and Bronx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Sorting Complaint type based on average request closing time\n",
    "\n",
    "# Calculate the average request closing time for each complaint type grouped by location\n",
    "avg_request_closing_time = (df.groupby(['City', 'Complaint Type'])['Closed Date']\n",
    "                            .apply(lambda x: (x - x.min()).mean()))\n",
    "\n",
    "# Sort the complaint types based on the average request closing time in ascending order\n",
    "sorted_complaints = avg_request_closing_time.sort_values()\n",
    "\n",
    "# Print the sorted complaints\n",
    "Average_ClosingTime = pd.DataFrame(sorted_complaints)\n",
    "Average_ClosingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Average response time across different complaint types\n",
    "\n",
    "# Calculate the overall average response time\n",
    "overall_avg_response_time = (df['Closed Date'] - df['Created Date']).mean()\n",
    "\n",
    "# Calculate the average response time for each complaint type\n",
    "avg_response_time_per_complaint = (df.groupby('Complaint Type').apply(lambda x: (x['Closed Date'] - x['Created Date']).mean())\n",
    "                                   .dt.total_seconds())\n",
    "\n",
    "# Print the average response time for each complaint type\n",
    "print(avg_response_time_per_complaint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average response time across the complaint types are similar, however, there are exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Visualise the average of Request Closing Time\n",
    "\n",
    "# Calculate the average request closing time per complaint type\n",
    "avg_request_closing_time = (df.groupby('Complaint Type').apply(lambda x: (x['Closed Date'] - x['Created Date']).mean())\n",
    "                                   .dt.total_seconds())\n",
    "\n",
    "# Sort the average closing time in descending order\n",
    "avg_request_closing_time = avg_request_closing_time.sort_values(ascending=False)\n",
    "\n",
    "# Plotting the average closing time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(avg_request_closing_time.index, avg_request_closing_time)\n",
    "plt.xlabel=('Complaint Type')\n",
    "plt.ylabel=('Average Request Closing Time')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot for average response time per complaint type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the average response time per complaint type in ascending order\n",
    "avg_response_time_per_complaint = avg_response_time_per_complaint.sort_values()\n",
    "\n",
    "# Perform statistical analysis and calculate p-values\n",
    "p_values = {}\n",
    "for complaint_type, response_time in avg_response_time_per_complaint.items():\n",
    "    sample = (df[df['Complaint Type'] == complaint_type]['Closed Date'] - df[df['Complaint Type'] == complaint_type]['Created Date']).dt.total_seconds()\n",
    "    _, p_value = stats.ttest_1samp(sample, overall_avg_response_time.total_seconds())\n",
    "    p_values[complaint_type] = p_value\n",
    "\n",
    "# Print only significant p-values\n",
    "for complaint_type, p_value in p_values.items():\n",
    "    if p_value is not None and p_value < 0.05:\n",
    "        print(f\"Complaint Type: {complaint_type}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer that variables with p-values <=0.05 indicates statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 perform Kruskal-Wallis H test\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate the response time for each complaint type\n",
    "response_times = {}\n",
    "for complaint_type, group in df.groupby('Complaint Type'):\n",
    "    response_times[complaint_type] = (group['Closed Date'] - group['Created Date']).dt.total_seconds()\n",
    "\n",
    "# Perform Kruskal-Wallis H test\n",
    "statistic, p_value = stats.kruskal(*response_times.values())\n",
    "\n",
    "# Print the result\n",
    "print(\"Kruskal-Wallis H Test:\")\n",
    "print(f\"Statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.1 fail to reject H0 : all sample distributions are equal\n",
    "\n",
    "#In this case, the low p-value indicates that the observed differences in the response times among the complaint types are unlikely to have occurred by chance alone. \n",
    "#The statistic value (11988.269402358468) represents the test statistic computed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.2 Reject H0:one or more sample distribution are not equal\n",
    "\n",
    "#In this case, the low p-value (close to zero) indicates that the observed differences in \n",
    "#the response times among the complaint types are highly unlikely to have occurred by chance alone. \n",
    "#The statistic value (11988.269402358468) represents the test statistic computed from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
